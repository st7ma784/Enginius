name: Model Training Pipeline

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      n_samples:
        description: 'Number of training samples'
        required: false
        default: '2000'
        type: string
      mcmc_samples:
        description: 'Number of MCMC samples'
        required: false
        default: '1000'
        type: string
      tune_hyperparams:
        description: 'Perform hyperparameter tuning'
        required: false
        default: false
        type: boolean

jobs:
  train-model:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Set up WandB
      env:
        WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
      run: |
        wandb login

    - name: Create output directories
      run: |
        mkdir -p outputs/models
        mkdir -p outputs/visualizations
        mkdir -p data

    - name: Train model
      env:
        WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        WANDB_PROJECT: car-collection-wait-times-ci
      run: |
        python main_training.py \
          --n_samples ${{ github.event.inputs.n_samples || '2000' }} \
          --mcmc_samples ${{ github.event.inputs.mcmc_samples || '1000' }} \
          --use_wandb \
          --wandb_project car-collection-wait-times-ci \
          --output_dir outputs \
          ${{ github.event.inputs.tune_hyperparams && '--tune_hyperparams' || '' }}

    - name: Generate model report
      run: |
        python -c "
        import json
        import os
        from datetime import datetime
        
        # Load results
        with open('outputs/results_summary.json', 'r') as f:
            results = json.load(f)
        
        # Generate report
        report = {
            'training_date': datetime.now().isoformat(),
            'git_sha': '${{ github.sha }}',
            'workflow_run': '${{ github.run_id }}',
            'parameters': {
                'n_samples': '${{ github.event.inputs.n_samples || "2000" }}',
                'mcmc_samples': '${{ github.event.inputs.mcmc_samples || "1000" }}',
                'tune_hyperparams': '${{ github.event.inputs.tune_hyperparams || "false" }}'
            },
            'performance': results['test_metrics']
        }
        
        with open('outputs/training_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        "

    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: trained-model-${{ github.run_id }}
        path: |
          outputs/models/
          outputs/visualizations/
          outputs/results_summary.json
          outputs/training_report.json
        retention-days: 30

    - name: Upload training data
      uses: actions/upload-artifact@v3
      with:
        name: training-data-${{ github.run_id }}
        path: outputs/training_data.csv
        retention-days: 7

    - name: Check model performance
      run: |
        python -c "
        import json
        
        with open('outputs/results_summary.json', 'r') as f:
            results = json.load(f)
        
        r2 = results['test_metrics']['r2']
        rmse = results['test_metrics']['rmse']
        
        print(f'Model Performance:')
        print(f'R² Score: {r2:.3f}')
        print(f'RMSE: {rmse:.2f} minutes')
        
        # Performance thresholds
        if r2 < 0.5:
            print('::warning::Model R² is below 0.5, consider investigating')
        if rmse > 30:
            print('::warning::Model RMSE is above 30 minutes, consider tuning')
        
        if r2 >= 0.7 and rmse <= 20:
            print('::notice::Model performance is good!')
        "

  notify-completion:
    runs-on: ubuntu-latest
    needs: train-model
    if: always()

    steps:
    - name: Notify on success
      if: needs.train-model.result == 'success'
      run: |
        echo "✅ Model training completed successfully!"
        echo "Artifacts are available in the workflow run."

    - name: Notify on failure
      if: needs.train-model.result == 'failure'
      run: |
        echo "❌ Model training failed!"
        echo "Please check the logs for details."